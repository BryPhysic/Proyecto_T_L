# -*- coding: utf-8 -*-
"""3_ner_avanzado_entitylinker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YjvqTtRsg0KU_lxWOSBP1-n49S0IN4to

# NER Avanzado con EntityLinker - Extracci√≥n de Entidades M√©dicas

**Asignatura:** Transformers del Lenguaje en Salud (UPCH)

## Objetivo

Crear un sistema completo de NER que:
- ‚úÖ Combina m√∫ltiples modelos (Hugging Face + scispacy)
- ‚úÖ Vincula entidades a bases de conocimiento m√©dico (UMLS, RxNorm)
- ‚úÖ Clasifica autom√°ticamente: Enfermedades, S√≠ntomas, Medicamentos
- ‚úÖ Enriquece con definiciones, sin√≥nimos y IDs normalizados
- ‚úÖ Detecta contexto (antecedentes, negaci√≥n, certeza)

## 1Ô∏è‚É£ Instalaci√≥n de Dependencias

**Nota:** La primera ejecuci√≥n tomar√° varios minutos debido a las descargas.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q matplotlib

# Commented out IPython magic to ensure Python compatibility.
# Instalaci√≥n de paquetes base
# %pip install -q transformers torch pandas matplotlib
# %pip install -q scispacy
# %pip install -q spacy>=3.7.0,<3.8.0

print("‚úÖ Paquetes base instalados")

# Commented out IPython magic to ensure Python compatibility.
# Descarga de modelos scispacy
# %pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_scibert-0.5.4.tar.gz
# %pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz

print("‚úÖ Modelos scispacy descargados")

"""## 2Ô∏è‚É£ Importaci√≥n de Librer√≠as"""

import json
import re
from datetime import datetime
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# Transformers (Hugging Face)
from transformers import pipeline

# Spacy y scispacy
import spacy
import scispacy
from scispacy.linking import EntityLinker
from scispacy.abbreviation import AbbreviationDetector

# Visualizaci√≥n
import pandas as pd
import matplotlib.pyplot as plt

print("‚úÖ Librer√≠as importadas correctamente")

"""## 3Ô∏è‚É£ Carga de Modelos NER

Cargamos m√∫ltiples modelos para maximizar la cobertura de entidades detectadas.
"""

# Modelo 1: Hugging Face - Biomedical NER (General)
print("‚è≥ Cargando modelo Hugging Face...")
ner_hf = pipeline(
    "token-classification",
    model="d4data/biomedical-ner-all",
    aggregation_strategy="simple"
)
print("‚úÖ Modelo Hugging Face listo")

# Modelo 2: scispacy - SciBERT (Base cient√≠fica)
print("‚è≥ Cargando scispacy SciBERT...")
nlp_scibert = spacy.load("en_core_sci_scibert")

# Agregar detector de abreviaturas
nlp_scibert.add_pipe("abbreviation_detector")
print("‚úÖ SciBERT + AbbreviationDetector listo")

# Modelo 3: scispacy - BC5CDR (Especialista en enfermedades y qu√≠micos)
print("‚è≥ Cargando scispacy BC5CDR...")
nlp_bc5cdr = spacy.load("en_ner_bc5cdr_md")
print("‚úÖ BC5CDR listo")

"""## 4Ô∏è‚É£ Configuraci√≥n de EntityLinker

**IMPORTANTE:** La primera ejecuci√≥n descargar√° ~1GB de datos de UMLS. Esto puede tardar varios minutos.
"""

# EntityLinker con UMLS (Base de conocimiento principal)
print("‚è≥ Configurando EntityLinker con UMLS...")
print("   (Primera vez: descarga de ~1GB, por favor espera)")

nlp_scibert.add_pipe(
    "scispacy_linker",
    config={
        "resolve_abbreviations": True,  # Expandir abreviaturas antes de vincular
        "linker_name": "umls",           # Base de conocimiento UMLS
        "threshold": 0.75,                # Umbral de confianza (balance)
        "k": 30,                          # Candidatos a considerar
        "max_entities_per_mention": 3     # Top 3 matches por entidad
    }
)

print("‚úÖ EntityLinker UMLS configurado")

# Acceso al linker para consultas posteriores
linker = nlp_scibert.get_pipe("scispacy_linker")
print(f"‚úÖ Knowledge base cargada: {len(linker.kb.cui_to_entity)} conceptos disponibles")

"""## 5Ô∏è‚É£ Definici√≥n de Tipos Sem√°nticos (TUI)

Clasificaci√≥n autom√°tica basada en los c√≥digos TUI de UMLS.
"""

# Mapeo de TUI (Type Unique Identifier) a categor√≠as
TUI_CATEGORIES = {
    "ENFERMEDAD": {
        "T047",  # Disease or Syndrome
        "T046",  # Pathologic Function
        "T048",  # Mental or Behavioral Dysfunction
        "T191",  # Neoplastic Process
        "T037",  # Injury or Poisoning
        "T049",  # Cell or Molecular Dysfunction
    },
    "SINTOMA": {
        "T184",  # Sign or Symptom
        "T033",  # Finding
        "T034",  # Laboratory or Test Result
    },
    "MEDICAMENTO": {
        "T121",  # Pharmacologic Substance
        "T109",  # Organic Chemical
        "T195",  # Antibiotic
        "T200",  # Clinical Drug
        "T114",  # Nucleic Acid, Nucleoside, or Nucleotide
    },
    "ANATOMIA": {
        "T029",  # Body Location or Region
        "T023",  # Body Part, Organ, or Organ Component
        "T030",  # Body Space or Junction
        "T024",  # Tissue
    },
    "PROCEDIMIENTO": {
        "T060",  # Diagnostic Procedure
        "T061",  # Therapeutic or Preventive Procedure
        "T059",  # Laboratory Procedure
    }
}

def clasificar_por_tui(tui_codes):
    """Clasifica una entidad bas√°ndose en sus c√≥digos TUI."""
    for categoria, tuis in TUI_CATEGORIES.items():
        if any(tui in tuis for tui in tui_codes):
            return categoria
    return "OTRO"

print("‚úÖ Sistema de clasificaci√≥n por TUI configurado")

"""## 6Ô∏è‚É£ Funciones de Extracci√≥n y Enriquecimiento"""

def extraer_entidades_multimodelo(texto):
    """
    Extrae entidades usando m√∫ltiples modelos y las consolida.

    Args:
        texto: Texto cl√≠nico a procesar

    Returns:
        dict con entidades de cada modelo y documento scispacy procesado
    """
    resultados = {}

    # 1. Hugging Face NER
    print("  üîç Ejecutando NER con Hugging Face...")
    resultados['hf'] = ner_hf(texto)

    # 2. scispacy SciBERT (con EntityLinker)
    print("  üîç Ejecutando NER con SciBERT + EntityLinker...")
    doc_scibert = nlp_scibert(texto)
    resultados['scibert'] = doc_scibert

    # 3. scispacy BC5CDR
    print("  üîç Ejecutando NER con BC5CDR...")
    doc_bc5cdr = nlp_bc5cdr(texto)
    resultados['bc5cdr'] = doc_bc5cdr

    return resultados

print("‚úÖ Funci√≥n de extracci√≥n multi-modelo definida")

def enriquecer_entidad(entidad_span, linker):
    """
    Enriquece una entidad con informaci√≥n de UMLS.

    Args:
        entidad_span: Span de spacy con la entidad
        linker: EntityLinker de scispacy

    Returns:
        dict con informaci√≥n enriquecida
    """
    entidad_info = {
        "texto_original": entidad_span.text,
        "tipo_ner": entidad_span.label_,
        "posicion": (entidad_span.start_char, entidad_span.end_char),
    }

    # Intentar obtener informaci√≥n de UMLS si est√° vinculada
    if hasattr(entidad_span._, 'kb_ents') and entidad_span._.kb_ents:
        # Tomar la mejor coincidencia (primera)
        umls_id, score = entidad_span._.kb_ents[0]

        # Obtener informaci√≥n completa de UMLS
        if umls_id in linker.kb.cui_to_entity:
            umls_entity = linker.kb.cui_to_entity[umls_id]

            entidad_info.update({
                "umls_id": umls_id,
                "nombre_normalizado": umls_entity.canonical_name,
                "definicion": umls_entity.definition if umls_entity.definition else "Sin definici√≥n disponible",
                "tipos_semanticos": list(umls_entity.types),
                "categoria": clasificar_por_tui(umls_entity.types),
                "sinonimos": list(umls_entity.aliases)[:5],  # Top 5 sin√≥nimos
                "score_linking": round(score, 3),
            })

            # Obtener todas las alternativas
            alternativas = []
            for alt_id, alt_score in entidad_span._.kb_ents[1:]:
                if alt_id in linker.kb.cui_to_entity:
                    alt_entity = linker.kb.cui_to_entity[alt_id]
                    alternativas.append({
                        "umls_id": alt_id,
                        "nombre": alt_entity.canonical_name,
                        "score": round(alt_score, 3)
                    })

            if alternativas:
                entidad_info["alternativas"] = alternativas

    return entidad_info

print("‚úÖ Funci√≥n de enriquecimiento definida")

def detectar_contexto(texto, entidad_texto, posicion):
    """
    Detecta el contexto de una entidad (temporal, negaci√≥n, certeza).

    Args:
        texto: Texto completo
        entidad_texto: Texto de la entidad
        posicion: Tupla (inicio, fin) de la entidad

    Returns:
        dict con informaci√≥n de contexto
    """
    inicio, fin = posicion

    # Obtener ventana de contexto (50 caracteres antes y despu√©s)
    ventana_inicio = max(0, inicio - 50)
    ventana_fin = min(len(texto), fin + 50)
    contexto = texto[ventana_inicio:ventana_fin].lower()

    resultado = {}

    # Detecci√≥n de temporalidad
    patrones_temporal = {
        "antecedente": ["antecedentes", "antecedente de", "historia de", "diagnosticado hace"],
        "actual": ["actualmente", "acude por", "presenta", "refiere", "motivo de consulta"],
        "pasado": ["hace", "desde hace", "a√±os de evoluci√≥n", "meses de evoluci√≥n", "d√≠as de evoluci√≥n"]
    }

    for tipo, patrones in patrones_temporal.items():
        if any(patron in contexto for patron in patrones):
            resultado["temporalidad"] = tipo
            break

    # Detecci√≥n de negaci√≥n
    patrones_negacion = ["niega", "sin", "no presenta", "se descarta", "negativo", "ausencia de"]
    if any(patron in contexto for patron in patrones_negacion):
        resultado["negacion"] = True
    else:
        resultado["negacion"] = False

    # Detecci√≥n de certeza
    if any(palabra in contexto for palabra in ["confirmado", "diagnosticado", "evidencia de"]):
        resultado["certeza"] = "confirmado"
    elif any(palabra in contexto for palabra in ["probable", "posible", "sospecha"]):
        resultado["certeza"] = "probable"
    else:
        resultado["certeza"] = "mencionado"

    return resultado

print("‚úÖ Funci√≥n de detecci√≥n de contexto definida")

"""## 7Ô∏è‚É£ Pipeline Completo de Procesamiento"""

def procesar_texto_clinico(texto):
    """
    Pipeline completo: extracci√≥n, enriquecimiento, clasificaci√≥n.

    Args:
        texto: Texto cl√≠nico a procesar

    Returns:
        dict con resultados estructurados y enriquecidos
    """
    print("\n" + "="*60)
    print("üöÄ INICIANDO PROCESAMIENTO NER AVANZADO")
    print("="*60)

    # Extracci√≥n multi-modelo
    resultados = extraer_entidades_multimodelo(texto)

    # Procesar entidades de SciBERT (tienen EntityLinker)
    doc_scibert = resultados['scibert']

    print(f"\n‚úÖ Detectadas {len(doc_scibert.ents)} entidades")

    # Estructura de salida
    output = {
        "metadatos": {
            "fecha_procesamiento": datetime.now().isoformat(),
            "modelos_usados": [
                "d4data/biomedical-ner-all",
                "en_core_sci_scibert",
                "en_ner_bc5cdr_md"
            ],
            "knowledge_base": "UMLS"
        },
        "texto_original": texto,
        "entidades_por_categoria": defaultdict(list),
        "abreviaturas": [],
        "estadisticas": defaultdict(int)
    }

    # Procesar abreviaturas
    print("\nüî§ Procesando abreviaturas...")
    for abrv in doc_scibert._.abbreviations:
        output["abreviaturas"].append({
            "abreviatura": abrv.text,
            "forma_larga": abrv._.long_form.text,
            "posicion": (abrv.start_char, abrv.end_char)
        })
    print(f"   Encontradas {len(output['abreviaturas'])} abreviaturas")

    # Procesar y enriquecer entidades
    print("\nüíé Enriqueciendo entidades con UMLS...")
    for ent in doc_scibert.ents:
        # Enriquecer con UMLS
        entidad_enriquecida = enriquecer_entidad(ent, linker)

        # Detectar contexto
        contexto = detectar_contexto(texto, ent.text, (ent.start_char, ent.end_char))
        entidad_enriquecida["contexto"] = contexto

        # Clasificar por categor√≠a
        categoria = entidad_enriquecida.get("categoria", "OTRO")
        output["entidades_por_categoria"][categoria].append(entidad_enriquecida)
        output["estadisticas"][categoria] += 1

    # Convertir defaultdict a dict normal para JSON
    output["entidades_por_categoria"] = dict(output["entidades_por_categoria"])
    output["estadisticas"] = dict(output["estadisticas"])

    print("\n" + "="*60)
    print("‚úÖ PROCESAMIENTO COMPLETADO")
    print("="*60)

    return output

print("‚úÖ Pipeline completo definido")

"""## 8Ô∏è‚É£ Texto Cl√≠nico de Prueba"""

# Historia cl√≠nica de ejemplo
historia_clinica = """
Paciente var√≥n de 65 a√±os, acude a emergencia por cuadro de 3 d√≠as de evoluci√≥n caracterizado por disnea de medianos esfuerzos y dolor tor√°cico opresivo.
Antecedentes: Hipertensi√≥n arterial diagnosticada hace 10 a√±os y Diabetes Mellitus tipo 2.
Actualmente en tratamiento con Losart√°n 50mg cada 12 horas y Metformina 850mg una vez al d√≠a.
Al examen f√≠sico: PA 150/90 mmHg, FC 95 lpm. Murmullo vesicular disminuido en bases.
Niega alergias a medicamentos conocidos. Se descarta infarto agudo de miocardio por enzimas cardiacas negativas.
"""

print(f"üìÑ Texto cl√≠nico cargado: {len(historia_clinica)} caracteres")

"""## 9Ô∏è‚É£ Ejecutar An√°lisis Completo"""

# Procesar el texto cl√≠nico
resultados = procesar_texto_clinico(historia_clinica)

"""## üîü Visualizaci√≥n de Resultados"""

# Mostrar estad√≠sticas generales
print("\n" + "="*60)
print("üìä ESTAD√çSTICAS GENERALES")
print("="*60)
print(f"\nüìÖ Fecha de procesamiento: {resultados['metadatos']['fecha_procesamiento']}")
print(f"üî§ Abreviaturas detectadas: {len(resultados['abreviaturas'])}")
print(f"\nüè∑Ô∏è  Entidades por categor√≠a:")
for categoria, cantidad in resultados['estadisticas'].items():
    print(f"   - {categoria}: {cantidad}")

# Mostrar abreviaturas detectadas
if resultados['abreviaturas']:
    print("\n" + "="*60)
    print("üî§ ABREVIATURAS DETECTADAS")
    print("="*60)
    for abrv in resultados['abreviaturas']:
        print(f"\n'{abrv['abreviatura']}' ‚Üí '{abrv['forma_larga']}'")

# Mostrar enfermedades detectadas
if "ENFERMEDAD" in resultados['entidades_por_categoria']:
    print("\n" + "="*60)
    print("üè• ENFERMEDADES DETECTADAS")
    print("="*60)

    for i, ent in enumerate(resultados['entidades_por_categoria']['ENFERMEDAD'], 1):
        print(f"\n[{i}] {ent['texto_original']}")
        if 'umls_id' in ent:
            print(f"    UMLS ID: {ent['umls_id']}")
            print(f"    Nombre normalizado: {ent['nombre_normalizado']}")
            print(f"    Tipo sem√°ntico: {', '.join(ent['tipos_semanticos'])}")
            print(f"    Confianza linking: {ent['score_linking']}")
            print(f"    Definici√≥n: {ent['definicion'][:150]}..." if len(ent['definicion']) > 150 else f"    Definici√≥n: {ent['definicion']}")
            if ent['contexto'].get('temporalidad'):
                print(f"    ‚è∞ Temporalidad: {ent['contexto']['temporalidad']}")
            if ent['contexto']['negacion']:
                print(f"    ‚ùå NEGADO")

# Mostrar s√≠ntomas detectados
if "SINTOMA" in resultados['entidades_por_categoria']:
    print("\n" + "="*60)
    print("ü©∫ S√çNTOMAS DETECTADOS")
    print("="*60)

    for i, ent in enumerate(resultados['entidades_por_categoria']['SINTOMA'], 1):
        print(f"\n[{i}] {ent['texto_original']}")
        if 'umls_id' in ent:
            print(f"    UMLS ID: {ent['umls_id']}")
            print(f"    Nombre normalizado: {ent['nombre_normalizado']}")
            print(f"    Confianza: {ent['score_linking']}")
            if ent['contexto'].get('temporalidad'):
                print(f"    ‚è∞ Temporalidad: {ent['contexto']['temporalidad']}")

# Mostrar medicamentos detectados
if "MEDICAMENTO" in resultados['entidades_por_categoria']:
    print("\n" + "="*60)
    print("üíä MEDICAMENTOS DETECTADOS")
    print("="*60)

    for i, ent in enumerate(resultados['entidades_por_categoria']['MEDICAMENTO'], 1):
        print(f"\n[{i}] {ent['texto_original']}")
        if 'umls_id' in ent:
            print(f"    UMLS ID: {ent['umls_id']}")
            print(f"    Nombre normalizado: {ent['nombre_normalizado']}")
            print(f"    Confianza: {ent['score_linking']}")
            print(f"    Sin√≥nimos: {', '.join(ent['sinonimos'][:3])}")

# Gr√°fico de distribuci√≥n de categor√≠as
if resultados['estadisticas']:
    plt.figure(figsize=(10, 6))
    categorias = list(resultados['estadisticas'].keys())
    cantidades = list(resultados['estadisticas'].values())

    plt.bar(categorias, cantidades, color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6'])
    plt.xlabel('Categor√≠a', fontsize=12)
    plt.ylabel('Cantidad de Entidades', fontsize=12)
    plt.title('Distribuci√≥n de Entidades Detectadas por Categor√≠a', fontsize=14, fontweight='bold')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.grid(axis='y', alpha=0.3)
    plt.show()

"""## 1Ô∏è‚É£1Ô∏è‚É£ Exportar Resultados"""

# Exportar a JSON
output_filename = 'ner_avanzado_resultados.json'

with open(output_filename, 'w', encoding='utf-8') as f:
    json.dump(resultados, f, ensure_ascii=False, indent=2)

print(f"\n‚úÖ Resultados exportados a: {output_filename}")
print(f"   Tama√±o del archivo: {len(json.dumps(resultados))} bytes")

# Mostrar JSON en formato legible (primeras l√≠neas)
print("\n" + "="*60)
print("üìÑ VISTA PREVIA JSON (primeras 50 l√≠neas)")
print("="*60 + "\n")

json_str = json.dumps(resultados, ensure_ascii=False, indent=2)
lineas = json_str.split('\n')
for linea in lineas[:100]:
    print(linea)

if len(lineas) > 100:
    print(f"\n... ({len(lineas) - 50} l√≠neas m√°s) ...")

"""## üéØ Resumen y Conclusiones

### ‚úÖ Logros

1. **Multi-modelo NER**: Combinaci√≥n de Hugging Face + scispacy para m√°xima cobertura
2. **EntityLinker UMLS**: Vinculaci√≥n a base de conocimiento m√©dico con 3M+ conceptos
3. **Clasificaci√≥n autom√°tica**: Por tipo sem√°ntico (TUI) - enfermedades, s√≠ntomas, medicamentos
4. **Enriquecimiento**: IDs normalizados, definiciones, sin√≥nimos
5. **Detecci√≥n de contexto**: Temporal, negaci√≥n, certeza
6. **Expansi√≥n de abreviaturas**: Autom√°tica con AbbreviationDetector

### üìä Aplicaciones

- **Estructuraci√≥n de historias cl√≠nicas**: Texto libre ‚Üí JSON estructurado
- **Sistemas de soporte diagn√≥stico**: Identificaci√≥n de entidades relevantes
- **Interoperabilidad**: IDs UMLS est√°ndares para integraci√≥n con otros sistemas
- **An√°lisis epidemiol√≥gico**: Extracci√≥n masiva de datos de historias cl√≠nicas
- **Investigaci√≥n cl√≠nica**: Identificaci√≥n de cohort es y patrones

### üöÄ Pr√≥ximos Pasos

1. A√±adir RxNorm para mejor cobertura de medicamentos
2. Implementar extracci√≥n de relaciones (medicamento ‚Üí enfermedad)
3. Fine-tuning de modelos con datos cl√≠nicos locales
4. Integraci√≥n con sistemas de historias cl√≠nicas electr√≥nicas
5. Validaci√≥n con conjunto de datos anotado manualmente
"""